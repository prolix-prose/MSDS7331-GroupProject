{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Online News Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Created by Phillip Efthimion, Scott Payne, Gino Varghese and John Blevins**\n",
    "\n",
    "*MSDS 7331 Data Mining - Section 403 - Lab 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1\t\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "Phillip\n",
    "* Copy data preparation steps\n",
    "* Copy Dimension Reduction and Scaling from Minilab (PCA)- this pickup up in Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7346 entries, 4 to 39639\n",
      "Data columns (total 53 columns):\n",
      "n_tokens_title                  7346 non-null float64\n",
      "n_tokens_content                7346 non-null float64\n",
      "n_unique_tokens                 7346 non-null float64\n",
      "n_non_stop_words                7346 non-null float64\n",
      "n_non_stop_unique_tokens        7346 non-null float64\n",
      "num_hrefs                       7346 non-null float64\n",
      "num_self_hrefs                  7346 non-null float64\n",
      "num_imgs                        7346 non-null float64\n",
      "num_videos                      7346 non-null float64\n",
      "average_token_length            7346 non-null float64\n",
      "num_keywords                    7346 non-null float64\n",
      "kw_min_min                      7346 non-null float64\n",
      "kw_max_min                      7346 non-null float64\n",
      "kw_avg_min                      7346 non-null float64\n",
      "kw_min_max                      7346 non-null float64\n",
      "kw_max_max                      7346 non-null float64\n",
      "kw_avg_max                      7346 non-null float64\n",
      "kw_min_avg                      7346 non-null float64\n",
      "kw_max_avg                      7346 non-null float64\n",
      "kw_avg_avg                      7346 non-null float64\n",
      "self_reference_min_shares       7346 non-null float64\n",
      "self_reference_max_shares       7346 non-null float64\n",
      "self_reference_avg_sharess      7346 non-null float64\n",
      "monday                          7346 non-null float64\n",
      "tuesday                         7346 non-null float64\n",
      "wednesday                       7346 non-null float64\n",
      "thursday                        7346 non-null float64\n",
      "friday                          7346 non-null float64\n",
      "saturday                        7346 non-null float64\n",
      "sunday                          7346 non-null float64\n",
      "weekend                         7346 non-null int64\n",
      "LDA_00                          7346 non-null float64\n",
      "LDA_01                          7346 non-null float64\n",
      "LDA_02                          7346 non-null float64\n",
      "LDA_03                          7346 non-null float64\n",
      "LDA_04                          7346 non-null float64\n",
      "global_subjectivity             7346 non-null float64\n",
      "global_sentiment_polarity       7346 non-null float64\n",
      "global_rate_positive_words      7346 non-null float64\n",
      "global_rate_negative_words      7346 non-null float64\n",
      "rate_positive_words             7346 non-null float64\n",
      "rate_negative_words             7346 non-null float64\n",
      "avg_positive_polarity           7346 non-null float64\n",
      "min_positive_polarity           7346 non-null float64\n",
      "max_positive_polarity           7346 non-null float64\n",
      "avg_negative_polarity           7346 non-null float64\n",
      "min_negative_polarity           7346 non-null float64\n",
      "max_negative_polarity           7346 non-null float64\n",
      "title_subjectivity              7346 non-null float64\n",
      "title_sentiment_polarity        7346 non-null float64\n",
      "abs_title_subjectivity          7346 non-null float64\n",
      "abs_title_sentiment_polarity    7346 non-null float64\n",
      "popularity                      7346 non-null int64\n",
      "dtypes: float64(51), int64(2)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Import and Configure Required Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "\n",
    "# Read Online News Data\n",
    "df = pd.read_csv('data/OnlineNewsPopularity.csv')\n",
    "\n",
    "# Correct Column Names by Removing Leading Space\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "\n",
    "# Rename Columns for Ease of Display\n",
    "df = df.rename(columns={'weekday_is_monday': 'monday', 'weekday_is_tuesday': 'tuesday', 'weekday_is_wednesday': 'wednesday', 'weekday_is_thursday': 'thursday', 'weekday_is_friday': 'friday', 'weekday_is_saturday': 'saturday', 'weekday_is_sunday': 'sunday', 'is_weekend': 'weekend'})\n",
    "df = df.rename(columns={'data_channel_is_lifestyle':'lifestyle', 'data_channel_is_entertainment':'entertainment', 'data_channel_is_bus':'business', 'data_channel_is_socmed':'social_media', 'data_channel_is_tech':'technology', 'data_channel_is_world':'world'})\n",
    "\n",
    "# Encode a new \"popular\" column based on the # of shares \n",
    "# \"popular\" = 1 and \"not popular\" to 0.\n",
    "df['popularity'] = pd.qcut(df['shares'].values, 2, labels=[0,1])\n",
    "df.popularity = df.popularity.astype(np.int)\n",
    "df.weekend = df.weekend.astype(np.int)\n",
    "\n",
    "\n",
    "# Take a subset of the data related to Technology News Articles\n",
    "dfsubset = df.loc[df['technology'] == 1]\n",
    "\n",
    "# Reassign to New Variable and remove Columns which aren't needed\n",
    "df_imputed = dfsubset\n",
    "del df_imputed['url']\n",
    "del df_imputed['shares']\n",
    "del df_imputed['timedelta']\n",
    "del df_imputed['lifestyle']\n",
    "del df_imputed['entertainment']\n",
    "del df_imputed['business']\n",
    "del df_imputed['social_media']\n",
    "del df_imputed['technology']\n",
    "del df_imputed['world']\n",
    "\n",
    "# Display Dataframe Structure\n",
    "df_imputed.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "Phillip\n",
    "* Describe how we created Popularity\n",
    "* Insert chart with attributes and meaning from lab 1\n",
    "The online news popularity data set utilized in this analysis is publicly accessible from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity.  The data was originally collected by Mashable based on articles published through their website between 2013 and 2015.  The data set contains 39,644 data points with 61 attributes including 58 predictive attributes, 2 non-predictive attributes and 1 target attribute.\n",
    "\n",
    "We created the ‘popularity' class variable using our data from our ‘shares’ variable. We have decided to measure how popular an article from the Mashable dataset is based on the number of shares it receives. If an article has been shared more than a requisite number of times, then it is deemed popular. We created popularity with the ‘qcut’ tool in pandas and split the ‘shares' into two categories: ‘popular’ and ‘not popular’, which are coded by 1 and 0 respectively. ‘Popularity’ is coded as a non-null integers, though most of the other variables are floats. \n",
    "\n",
    "The 2 non-predictive attributes are as follows:\n",
    "* URL of the news article\n",
    "* Number of days between article publication and dataset acquisition\n",
    "\n",
    "The explanatory attributes can be split into several different groupings as follows:\n",
    "* Word Structure and Frequency\n",
    "* Hyperlink References\n",
    "* Digital Media References (Images and Videos)\n",
    "* Channel Categorization and Keywords (content type such as lifestyle, entertainment, etc...)\n",
    "* Publication Time\n",
    "* Sentiment and Subjectivity Levels\n",
    "\n",
    "The target attribute is the number of shares a site receives which indicates the popularity of the site.  The complete list of attributes is shown in the following table:\n",
    "\n",
    "|Attribute|Data Type|Description|\n",
    "|---------|---------|-----------|\n",
    "| url | Object | URL of the article (non-predictive) |\n",
    "| timedelta | Float | Days between the article publication and the dataset acquisition (non-predictive) |\n",
    "| n_tokens_title | Float | Number of words in the title |\n",
    "| n_tokens_content | Float | Number of words in the content |\n",
    "| n_unique_tokens | Float | Rate of unique words in the content |\n",
    "| n_non_stop_words | Float | Rate of non-stop words in the content |\n",
    "| n_non_stop_unique_tokens | Float | Rate of unique non-stop words in the content |\n",
    "| num_hrefs | Float | Number of links |\n",
    "| num_self_hrefs | Float | Number of links to other articles published by Mashable |\n",
    "| num_imgs | Float | Number of images |\n",
    "| num_videos | Float | Number of videos |\n",
    "| average_token_length | Float | Average length of the words in the content |\n",
    "| num_keywords | Float | Number of keywords in the metadata |\n",
    "| data_channel_is_lifestyle | Float | Is data channel 'Lifestyle'? |\n",
    "| data_channel_is_entertainment | Float | Is data channel 'Entertainment'? |\n",
    "| data_channel_is_bus | Float | Is data channel 'Business'? |\n",
    "| data_channel_is_socmed | Float | Is data channel 'Social Media'? |\n",
    "| data_channel_is_tech | Float | Is data channel 'Tech'? |\n",
    "| data_channel_is_world | Float | Is data channel 'World'? |\n",
    "| kw_min_min | Float | Worst keyword (min) |\n",
    "| kw_max_min | Float | Worst keyword (max) |\n",
    "| kw_avg_min | Float | Worst keyword (avg) |\n",
    "| kw_min_max | Float | Best keyword (min) |\n",
    "| kw_max_max | Float | Best keyword (max) |\n",
    "| kw_avg_max | Float | Best keyword (avg) |\n",
    "| kw_min_avg | Float | Avg keyword (min) |\n",
    "| kw_max_avg | Float | Avg keyword (max) |\n",
    "| kw_avg_avg | Float | Avg keyword (avg) |\n",
    "| self_reference_min_shares | Float | Min  of referenced articles in Mashable |\n",
    "| self_reference_max_shares | Float | Max  of referenced articles in Mashable |\n",
    "| self_reference_avg_sharess | Float | Avg  of referenced articles in Mashable |\n",
    "| weekday_is_monday | Float | Was the article published on a Monday? |\n",
    "| weekday_is_tuesday | Float | Was the article published on a Tuesday? |\n",
    "| weekday_is_wednesday | Float | Was the article published on a Wednesday? |\n",
    "| weekday_is_thursday | Float | Was the article published on a Thursday? |\n",
    "| weekday_is_friday | Float | Was the article published on a Friday? |\n",
    "| weekday_is_saturday | Float | Was the article published on a Saturday? |\n",
    "| weekday_is_sunday | Float | Was the article published on a Sunday? |\n",
    "| is_weekend | Float | Was the article published on the weekend? |\n",
    "| LDA_00 | Float | Closeness to LDA topic 0 |\n",
    "| LDA_01 | Float | Closeness to LDA topic 1 |\n",
    "| LDA_02 | Float | Closeness to LDA topic 2 |\n",
    "| LDA_03 | Float | Closeness to LDA topic 3 |\n",
    "| LDA_04 | Float | Closeness to LDA topic 4 |\n",
    "| global_subjectivity | Float | Text subjectivity |\n",
    "| global_sentiment_polarity | Float | Text sentiment polarity |\n",
    "| global_rate_positive_words | Float | Rate of positive words in the content |\n",
    "| global_rate_negative_words | Float | Rate of negative words in the content |\n",
    "| rate_positive_words | Float | Rate of positive words among non-neutral tokens |\n",
    "| rate_negative_words | Float | Rate of negative words among non-neutral tokens |\n",
    "| avg_positive_polarity | Float | Avg polarity of positive words |\n",
    "| min_positive_polarity | Float | Min polarity of positive words |\n",
    "| max_positive_polarity | Float | Max polarity of positive words |\n",
    "| avg_negative_polarity | Float | Avg polarity of positive words |\n",
    "| min_negative_polarity | Float | Min polarity of positive words |\n",
    "| max_negative_polarity | Float | Max polarity of positive words |\n",
    "| title_subjectivity | Float | Title subjectivity |\n",
    "| title_sentiment_polarity | Float | Title polarity |\n",
    "| abs_title_subjectivity | Float | Absolute subjectivity level |\n",
    "| abs_title_sentiment_polarity | Float | Absolute polarity level |\n",
    "| Number of shares (target) | Integer | Number of Article Shares (tweets, shares, etc...)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "Phillip\n",
    "* Calculating Acc, Rec, F-measure, Negative Predictive Value, Specificity etc... from confusion matrix\n",
    "\n",
    "Gino - Saturday Afternoon\n",
    "\n",
    "Final attributes from dimension reduction:\n",
    "['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']\n",
    "\n",
    "* 2 Tasks: Popularity (classification) and Share # (Regression)\n",
    "* Regression - Linear, Lasso, Ridge\n",
    "* Classification - Logistic Regression, K nearest Neighbors, Random Forest\n",
    "\n",
    "\n",
    "* Accuracy\n",
    "Accuracy : the proportion of the total number of predictions that were correct.\n",
    "(a + d) / (a + b + c + d)\n",
    "\n",
    "* Precision\n",
    "Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "a / (a + b)\n",
    "Negative Predictive Value : the proportion of negative cases that were correctly identified.\n",
    "d / (c + d)\n",
    "\n",
    "\n",
    "\n",
    "* Recall\n",
    "Recall : the proportion of actual positive cases which are correctly identified.\n",
    "a / (a + c)\n",
    "\n",
    "Recall could be our primary go to metric as it evaluates the context of identifying online articles which fall between High to Medium popularity or Medium to Low popularity(ex. articles that are on the border line between popularity classes)\n",
    "\n",
    "* F-Measure\n",
    "the proportion of actual negative cases which are correctly identified.\n",
    "d / (b + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import metrics to collect metrics for each modelS\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "#Separating data sets for each task\n",
    "\n",
    "#task 1 Popularity Classification\n",
    "#df_task1 = df_imputed[['popularity','n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']].copy()\n",
    "df_task1=df_imputed\n",
    "\n",
    "#task 2 isWeekend Classification\n",
    "df_task2=df_imputed\n",
    "df_task2 = df_task2.drop(['monday', 'tuesday','wednesday','thursday','friday','saturday','sunday'], axis=1)\n",
    " \n",
    "\n",
    "#Task 1\n",
    "#Create list to store datapoints for accuracy, precision, recall, F-measure from each of the model\n",
    "accuracy_task1 = []\n",
    "precision_task1 = []\n",
    "recall_task1 = []\n",
    "fmeasure_task1 = []\n",
    "\n",
    "#Task 2\n",
    "#Create list to store datapoints for accuracy, precision, recall, F-measure from each of the model\n",
    "accuracy_task2 = []\n",
    "precision_task2 = []\n",
    "recall_task2 = []\n",
    "fmeasure_task2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2\n",
    "Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "Gino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our approach we will be using Stratified 10-Fold Cross Validation for our analysis. \n",
    "- This approach is used to obtain a sample population that best represents the entire population being analysed. Some of the advantages include but not limited to, minimizing sample selection bias and ensuring certain segments of the population not overrepresented or underrepresented, botom line is Stratification is the process of rearranging the data as to ensure each fold is a good representative of the whole. In our process, the data will be rearranged 10 times and it is most appropriate for us to use in our analysis due to the characteristic of our data set.         \n",
    "<br>\n",
    "- As the data in our data set was collected to summarizes a heterogeneous set of features about articles published by Mashable in a period of two years and because it includes such a diverse sample of individuals, it is very likely that the features that are highly correlated to our target classes. The locality of our testing data should not influence the classification of individual testing data. This is very important to the model we design. By performing a Stratified 10-Fold Cross Validation not only will provide us with a thorough method of splitting, training, and testing our models against our data set but also ensures our model to maintain a similar level of accuracy as it was developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "Gino\n",
    "\n",
    "* For Regression make sure popularity field is excluded and for classification make sure Share # is excluded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Popularity (classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stratified 10 folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_task1_temp=df_task1\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'popularity' in df_task1_temp:\n",
    "    y_1 = df_task1_temp['popularity'].values # get the labels we want\n",
    "    del df_task1_temp['popularity'] # get rid of the class label\n",
    "    X_1 = df_task1_temp.values # use everything else to predict!\n",
    "    \n",
    "yhat_1 = np.zeros(y.shape)\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "                         \n",
    "print(cv_object)\n",
    "cv_object.get_n_splits(X_1,y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 61.3940%\n",
      "Recall of the model: 89.6306%\n",
      "Precision of the model: 62.1046%\n",
      "F-measure of the model: 73.3709%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "#penalty is set to default, which is 12.\n",
    "def lr_explor(cost):\n",
    "    lr_task1 = LogisticRegression(C=cost, class_weight=None)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_1,y_1):\n",
    "        lr_task1.fit(X_1[train],y_1[train])\n",
    "        yhat_1[test] = lr_task1.predict(X_1[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y_1, yhat_1)\n",
    "    recall = mt.recall_score(y_1, yhat_1)\n",
    "    precision = mt.precision_score(y_1, yhat_1)\n",
    "    f = mt.f1_score(y_1, yhat_1)\n",
    "\n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task1.append(['logistic_accuracy',acc])\n",
    "recall_task1.append(['logistic_recall',recall])\n",
    "precision_task1.append(['logistic_precision',precision])\n",
    "fmeasure_task1.append(['logistic_F-Measure',f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy returned by the classifier is: 58.2538% with k value of: 21\n"
     ]
    }
   ],
   "source": [
    "# As a team we setup KNN Classifier iterator to to determine the accurate number of nearest neighbours\n",
    "# the highest iterations we are planning was 30, to get the best accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "counter = 1;\n",
    "best_accuracy= 0.0;\n",
    "kVal = 1;\n",
    "while counter <= 30:\n",
    "    clf = KNeighborsClassifier(n_neighbors=counter)\n",
    "    clf.fit(X_1[train],y_1[train])\n",
    "    acc = clf.score(X_1[test],y_1[test]);\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc;\n",
    "        kVal = counter;\n",
    "    counter += 1;\n",
    "neighbors=kVal\n",
    "print(\"Best Accuracy returned by the classifier is: {0:.4f}%\".format(best_accuracy*100),\"with k value of:\",kVal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best k value: 21\n",
      "Accuracy of the model: 58.0860%\n",
      "Recall of the model: 75.6596%\n",
      "Precision of the model: 62.0391%\n",
      "F-measure of the model: 68.1757%\n"
     ]
    }
   ],
   "source": [
    "# Actual trainning and testing of the model begins\n",
    "print(\"The best k value:\", neighbors)\n",
    "knn_task1 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X_1,y_1):\n",
    "    knn_task1.fit(X_1[train],y_1[train])\n",
    "    yhat_1[test] = knn_task1.predict(X_1[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y_1, yhat_1)\n",
    "recall = mt.recall_score(y_1, yhat_1)\n",
    "precision = mt.precision_score(y_1, yhat_1)\n",
    "f = mt.f1_score(y_1, yhat_1)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task1.append(['KNN',acc])\n",
    "recall_task1.append(['KNN',recall])\n",
    "precision_task1.append(['KNN',precision])\n",
    "fmeasure_task1.append(['KNN',f])\n",
    "\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 65.0422%\n",
      "Recall of the model: 81.5095%\n",
      "Precision of the model: 66.8485%\n",
      "F-measure of the model: 73.4546%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def n_estimator(num):\n",
    "    # get a handle to the classifier object, which defines the type\n",
    "    rf_task1 = RandomForestClassifier(n_estimators=num, n_jobs=-1)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_1,y_1):\n",
    "        rf_task1.fit(X_1[train],y_1[train])\n",
    "        yhat_1[test] = rf_task1.predict(X_1[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y_1, yhat_1)\n",
    "    recall = mt.recall_score(y_1, yhat_1)\n",
    "    precision = mt.precision_score(y_1, yhat_1)\n",
    "    f = mt.f1_score(y_1, yhat_1)\n",
    "\n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "wd.interact(n_estimator,num=(100,150,10),__manual=True) \n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task1.append(['RandomForest',acc])\n",
    "recall_task1.append(['RandomForest',recall])\n",
    "precision_task1.append(['RandomForest',precision])\n",
    "fmeasure_task1.append(['RandomForest',f])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Weekend (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_object=None\n",
    "df_task2_temp=df_task2\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'weekend' in df_task2_temp:\n",
    "    y_2 = df_task2_temp['weekend'].values # get the labels we want\n",
    "    del df_task2_temp['weekend'] # get rid of the class label\n",
    "    X_2 = df_task2_temp.values # use everything else to predict!\n",
    "    \n",
    "yhat_2 = np.zeros(y.shape)\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "                         \n",
    "print(cv_object)\n",
    "cv_object.get_n_splits(X_2,y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 86.9589%\n",
      "Recall of the model: 0.0000%\n",
      "Precision of the model: 0.0000%\n",
      "F-measure of the model: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost_2):\n",
    "# get a handle to the classifier object, which defines the type\n",
    "#penalty is set to default, which is 12.\n",
    "    lr_task2 = LogisticRegression(C=cost_2, class_weight=None)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_2,y_2):\n",
    "        lr_task2.fit(X_2[train],y_2[train])\n",
    "        yhat_2[test] = lr_task2.predict(X_2[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y_2, yhat_2)\n",
    "    recall = mt.recall_score(y_2, yhat_2)\n",
    "    precision = mt.precision_score(y_2, yhat_2)\n",
    "    f = mt.f1_score(y_2, yhat_2)\n",
    "    \n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "    \n",
    "wd.interact(lr_explor,cost_2=(0.001,5.0,0.05),__manual=True) \n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task2.append(['logistic_accuracy',acc])\n",
    "recall_task2.append(['logistic_recall',recall])\n",
    "precision_task2.append(['logistic_precision',precision])\n",
    "fmeasure_task2.append(['logistic_F-Measure',f])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy returned by the classifier is: 87.4488% with k value of: 16\n"
     ]
    }
   ],
   "source": [
    "# As a team we setup KNN Classifier iterator to to determine the accurate number of nearest neighbours\n",
    "# the highest iterations we are planning was 30, to get the best accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "counter = 1;\n",
    "best_accuracy= 0.0;\n",
    "kVal = 1;\n",
    "while counter <= 30:\n",
    "    clf = KNeighborsClassifier(n_neighbors=counter)\n",
    "    clf.fit(X_2[train],y_2[train])\n",
    "    acc = clf.score(X_2[test],y_2[test]);\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc;\n",
    "        kVal = counter;\n",
    "    counter += 1;\n",
    "neighbors=kVal\n",
    "print(\"Best Accuracy returned by the classifier is: {0:.4f}%\".format(best_accuracy*100),\"with k value of:\",kVal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best k value: 16\n",
      "Accuracy of the model: 87.4217%\n",
      "Recall of the model: 0.5429%\n",
      "Precision of the model: 38.4615%\n",
      "F-measure of the model: 1.0707%\n"
     ]
    }
   ],
   "source": [
    "# Actual trainning and testing of the model begins\n",
    "print(\"The best k value:\", neighbors)\n",
    "knn_task2 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X_2,y_2):\n",
    "    knn_task2.fit(X_2[train],y_2[train])\n",
    "    yhat_2[test] = knn_task2.predict(X_2[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y_2, yhat_2)\n",
    "recall = mt.recall_score(y_2, yhat_2)\n",
    "precision = mt.precision_score(y_2, yhat_2)\n",
    "f = mt.f1_score(y_2, yhat_2)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task2.append(['KNN',acc])\n",
    "recall_task2.append(['KNN',recall])\n",
    "precision_task2.append(['KNN',precision])\n",
    "fmeasure_task2.append(['KNN',f])\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 88.8375%\n",
      "Recall of the model: 12.0521%\n",
      "Precision of the model: 91.7355%\n",
      "F-measure of the model: 21.3052%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def n_estimator(num):\n",
    "    # get a handle to the classifier object, which defines the type\n",
    "    rf_task2 = RandomForestClassifier(n_estimators=num, n_jobs=-1)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_2,y_2):\n",
    "        rf_task2.fit(X_2[train],y_2[train])\n",
    "        yhat_2[test] = rf_task2.predict(X_2[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y, yhat)\n",
    "    recall = mt.recall_score(y, yhat)\n",
    "    precision = mt.precision_score(y, yhat)\n",
    "    f = mt.f1_score(y, yhat)\n",
    "\n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "wd.interact(n_estimator,num=(100,150,10),__manual=True) \n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task2.append(['RandomForest',acc])\n",
    "recall_task2.append(['RandomForest',recall])\n",
    "precision_task2.append(['RandomForest',precision])\n",
    "fmeasure_task2.append(['RandomForest',f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "John\n",
    "* For Regression just compare attributes selected which are significantly\n",
    "* For Classification determine accuracy, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "John\n",
    "Notebook - Grand Puba Notebook\n",
    "Rsquared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "John\n",
    "* Discuss weights in more detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? \n",
    "\n",
    "Scott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "\n",
    "Scott\n",
    "\n",
    "Possible Analysis:\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "Neaural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "0687b94527184dfb89c70cf1ff999470": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "81fda56fe4074db797d88e7d9d317fd4": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "c95a2cf90dad42539cf92903050455f1": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "f2833ada527b49768c0cced447e51548": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
