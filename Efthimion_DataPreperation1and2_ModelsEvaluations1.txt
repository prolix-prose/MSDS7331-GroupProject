
# Lab 2

# Data Preparation 1

The online news popularity data set utilized in this analysis is publicly accessible from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity. The data was originally collected by Mashable based on articles published through their website between 2013 and 2015. The data set contains 39,644 data points with 61 attributes including 58 predictive attributes, 2 non-predictive attributes and 1 target attribute.

The data set is not a raw dataset and has already been cleaned prior to its upload on the UCI Machine Learning Repository. As a result there are no missing values that need to be reconciled.


In this lab, we will be be looking at the classifications of how popular and article is and whether it was published on a weekday or weekend. We will be analyzing our data using logistic regression, k nearest neighbors, random forest, and deep learning methods. We will be evaluating the output of these methods based on their accuracy, precision, recall, and F-measure. 

To examine the data in more detail and perform subsequent analysis operations the following modules are imported:

Pandas
Numpy
Matplotlib
Seaborn
Warnings
Datetime
ipywidgets


##############################
##############################
##############################

# Data Preparation 2

The weekend classification is a non-null float64 attribute. I was included as part of our dataset. It is a binary variable that tells if the article of this instance was published on the weekend or not. Articles not published on the weekend are published on weekdays. In this attribute, an instance is coded as 1 if it occurs on the weekend and an instance is coded as 0 if it occurs on a weekday. 


##############################
##############################
##############################

# Modeling and Evaluation 1

Accuracy is the proportion of the total number of predictions that were correct. From a confusion matrix, we can obtain the accuracy of the model by adding the number of instances that were predicted to be ‘Yes' and were actually ‘Yes' by the number of instances that were predicted to be ’No' and actually were ’No'. Then we take that sum and divide it by the total number of instances. Therefore the accuracy is measuring what percentage of the instances were identified ‘Yes’. 

Precision is the proportion of the positive cases that were identified ‘Yes'. From a confusion matrix, we can obtain the precision of the model by taking the number of instances that were predicted to be ‘Yes' and were actually ‘Yes' and divide this by the total number of instances that were predicted to be ‘Yes'. 

The Recall, also known as True Positive Rate, tells us that when an instance is actually ‘Yes', how often is it predicted to be ‘Yes’. So Recall is taking the number of instances that are predicted to be ‘Yes’ and are actually ‘Yes’ and dividing that by the total number of instances that are actually ‘Yes’. 

The F-measure is a weighted average of the precision and recall. The F measure is calculated by the product of the precision and recall divided by the sum of the precision and recall. The F measure is scored between 0 and 1 with 1 being the best value and 0 being the lowest. 

All of these measurements are appropriate for measuring our data. The accuracy lets us know a proportion of how well our model is performing to correctly predict the categories that our data belongs in. The precision and recall are both also appropriate measurements. To tell us what percentage of Mashable articles that we predict will be popular and are actually popular, precision is required. To tell us that when an article is actual popular, what percentage of time was this predicted? The F measure is a great tool for measuring the performance of models using the precision and recall, which we are already calculating both. Therefore, these are all appropriate models.
