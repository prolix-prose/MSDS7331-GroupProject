{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Online News Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Created by Phillip Efthimion, Scott Payne, Gino Varghese and John Blevins**\n",
    "\n",
    "*MSDS 7331 Data Mining - Section 403 - Lab 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1\t\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "Phillip\n",
    "* Copy data preparation steps\n",
    "* Copy Dimension Reduction and Scaling from Minilab (PCA)- this pickup up in Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7346 entries, 4 to 39639\n",
      "Data columns (total 53 columns):\n",
      "n_tokens_title                  7346 non-null float64\n",
      "n_tokens_content                7346 non-null float64\n",
      "n_unique_tokens                 7346 non-null float64\n",
      "n_non_stop_words                7346 non-null float64\n",
      "n_non_stop_unique_tokens        7346 non-null float64\n",
      "num_hrefs                       7346 non-null float64\n",
      "num_self_hrefs                  7346 non-null float64\n",
      "num_imgs                        7346 non-null float64\n",
      "num_videos                      7346 non-null float64\n",
      "average_token_length            7346 non-null float64\n",
      "num_keywords                    7346 non-null float64\n",
      "kw_min_min                      7346 non-null float64\n",
      "kw_max_min                      7346 non-null float64\n",
      "kw_avg_min                      7346 non-null float64\n",
      "kw_min_max                      7346 non-null float64\n",
      "kw_max_max                      7346 non-null float64\n",
      "kw_avg_max                      7346 non-null float64\n",
      "kw_min_avg                      7346 non-null float64\n",
      "kw_max_avg                      7346 non-null float64\n",
      "kw_avg_avg                      7346 non-null float64\n",
      "self_reference_min_shares       7346 non-null float64\n",
      "self_reference_max_shares       7346 non-null float64\n",
      "self_reference_avg_sharess      7346 non-null float64\n",
      "monday                          7346 non-null float64\n",
      "tuesday                         7346 non-null float64\n",
      "wednesday                       7346 non-null float64\n",
      "thursday                        7346 non-null float64\n",
      "friday                          7346 non-null float64\n",
      "saturday                        7346 non-null float64\n",
      "sunday                          7346 non-null float64\n",
      "weekend                         7346 non-null int32\n",
      "LDA_00                          7346 non-null float64\n",
      "LDA_01                          7346 non-null float64\n",
      "LDA_02                          7346 non-null float64\n",
      "LDA_03                          7346 non-null float64\n",
      "LDA_04                          7346 non-null float64\n",
      "global_subjectivity             7346 non-null float64\n",
      "global_sentiment_polarity       7346 non-null float64\n",
      "global_rate_positive_words      7346 non-null float64\n",
      "global_rate_negative_words      7346 non-null float64\n",
      "rate_positive_words             7346 non-null float64\n",
      "rate_negative_words             7346 non-null float64\n",
      "avg_positive_polarity           7346 non-null float64\n",
      "min_positive_polarity           7346 non-null float64\n",
      "max_positive_polarity           7346 non-null float64\n",
      "avg_negative_polarity           7346 non-null float64\n",
      "min_negative_polarity           7346 non-null float64\n",
      "max_negative_polarity           7346 non-null float64\n",
      "title_subjectivity              7346 non-null float64\n",
      "title_sentiment_polarity        7346 non-null float64\n",
      "abs_title_subjectivity          7346 non-null float64\n",
      "abs_title_sentiment_polarity    7346 non-null float64\n",
      "popularity                      7346 non-null int32\n",
      "dtypes: float64(51), int32(2)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Import and Configure Required Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "from tabulate import tabulate\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "\n",
    "# Read Online News Data\n",
    "df = pd.read_csv('data/OnlineNewsPopularity.csv')\n",
    "\n",
    "# Correct Column Names by Removing Leading Space\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "\n",
    "# Rename Columns for Ease of Display\n",
    "df = df.rename(columns={'weekday_is_monday': 'monday', 'weekday_is_tuesday': 'tuesday', 'weekday_is_wednesday': 'wednesday', 'weekday_is_thursday': 'thursday', 'weekday_is_friday': 'friday', 'weekday_is_saturday': 'saturday', 'weekday_is_sunday': 'sunday', 'is_weekend': 'weekend'})\n",
    "df = df.rename(columns={'data_channel_is_lifestyle':'lifestyle', 'data_channel_is_entertainment':'entertainment', 'data_channel_is_bus':'business', 'data_channel_is_socmed':'social_media', 'data_channel_is_tech':'technology', 'data_channel_is_world':'world'})\n",
    "\n",
    "# Encode a new \"popular\" column based on the # of shares \n",
    "# \"popular\" = 1 and \"not popular\" to 0.\n",
    "df['popularity'] = pd.qcut(df['shares'].values, 2, labels=[0,1])\n",
    "df.popularity = df.popularity.astype(np.int)\n",
    "df.weekend = df.weekend.astype(np.int)\n",
    "\n",
    "\n",
    "# Take a subset of the data related to Technology News Articles\n",
    "dfsubset = df.loc[df['technology'] == 1]\n",
    "\n",
    "# Reassign to New Variable and remove Columns which aren't needed\n",
    "df_imputed = dfsubset\n",
    "del df_imputed['url']\n",
    "del df_imputed['shares']\n",
    "del df_imputed['timedelta']\n",
    "del df_imputed['lifestyle']\n",
    "del df_imputed['entertainment']\n",
    "del df_imputed['business']\n",
    "del df_imputed['social_media']\n",
    "del df_imputed['technology']\n",
    "del df_imputed['world']\n",
    "\n",
    "# Display Dataframe Structure\n",
    "df_imputed.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "Phillip\n",
    "* Describe how we created Popularity\n",
    "* Insert chart with attributes and meaning from lab 1\n",
    "The online news popularity data set utilized in this analysis is publicly accessible from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity.  The data was originally collected by Mashable based on articles published through their website between 2013 and 2015.  The data set contains 39,644 data points with 61 attributes including 58 predictive attributes, 2 non-predictive attributes and 1 target attribute.\n",
    "\n",
    "We created the ‘popularity' class variable using our data from our ‘shares’ variable. We have decided to measure how popular an article from the Mashable dataset is based on the number of shares it receives. If an article has been shared more than a requisite number of times, then it is deemed popular. We created popularity with the ‘qcut’ tool in pandas and split the ‘shares' into two categories: ‘popular’ and ‘not popular’, which are coded by 1 and 0 respectively. ‘Popularity’ is coded as a non-null integers, though most of the other variables are floats. \n",
    "\n",
    "The 2 non-predictive attributes are as follows:\n",
    "* URL of the news article\n",
    "* Number of days between article publication and dataset acquisition\n",
    "\n",
    "The explanatory attributes can be split into several different groupings as follows:\n",
    "* Word Structure and Frequency\n",
    "* Hyperlink References\n",
    "* Digital Media References (Images and Videos)\n",
    "* Channel Categorization and Keywords (content type such as lifestyle, entertainment, etc...)\n",
    "* Publication Time\n",
    "* Sentiment and Subjectivity Levels\n",
    "\n",
    "The target attribute is the number of shares a site receives which indicates the popularity of the site.  The complete list of attributes is shown in the following table:\n",
    "\n",
    "|Attribute|Data Type|Description|\n",
    "|---------|---------|-----------|\n",
    "| url | Object | URL of the article (non-predictive) |\n",
    "| timedelta | Float | Days between the article publication and the dataset acquisition (non-predictive) |\n",
    "| n_tokens_title | Float | Number of words in the title |\n",
    "| n_tokens_content | Float | Number of words in the content |\n",
    "| n_unique_tokens | Float | Rate of unique words in the content |\n",
    "| n_non_stop_words | Float | Rate of non-stop words in the content |\n",
    "| n_non_stop_unique_tokens | Float | Rate of unique non-stop words in the content |\n",
    "| num_hrefs | Float | Number of links |\n",
    "| num_self_hrefs | Float | Number of links to other articles published by Mashable |\n",
    "| num_imgs | Float | Number of images |\n",
    "| num_videos | Float | Number of videos |\n",
    "| average_token_length | Float | Average length of the words in the content |\n",
    "| num_keywords | Float | Number of keywords in the metadata |\n",
    "| data_channel_is_lifestyle | Float | Is data channel 'Lifestyle'? |\n",
    "| data_channel_is_entertainment | Float | Is data channel 'Entertainment'? |\n",
    "| data_channel_is_bus | Float | Is data channel 'Business'? |\n",
    "| data_channel_is_socmed | Float | Is data channel 'Social Media'? |\n",
    "| data_channel_is_tech | Float | Is data channel 'Tech'? |\n",
    "| data_channel_is_world | Float | Is data channel 'World'? |\n",
    "| kw_min_min | Float | Worst keyword (min) |\n",
    "| kw_max_min | Float | Worst keyword (max) |\n",
    "| kw_avg_min | Float | Worst keyword (avg) |\n",
    "| kw_min_max | Float | Best keyword (min) |\n",
    "| kw_max_max | Float | Best keyword (max) |\n",
    "| kw_avg_max | Float | Best keyword (avg) |\n",
    "| kw_min_avg | Float | Avg keyword (min) |\n",
    "| kw_max_avg | Float | Avg keyword (max) |\n",
    "| kw_avg_avg | Float | Avg keyword (avg) |\n",
    "| self_reference_min_shares | Float | Min  of referenced articles in Mashable |\n",
    "| self_reference_max_shares | Float | Max  of referenced articles in Mashable |\n",
    "| self_reference_avg_sharess | Float | Avg  of referenced articles in Mashable |\n",
    "| weekday_is_monday | Float | Was the article published on a Monday? |\n",
    "| weekday_is_tuesday | Float | Was the article published on a Tuesday? |\n",
    "| weekday_is_wednesday | Float | Was the article published on a Wednesday? |\n",
    "| weekday_is_thursday | Float | Was the article published on a Thursday? |\n",
    "| weekday_is_friday | Float | Was the article published on a Friday? |\n",
    "| weekday_is_saturday | Float | Was the article published on a Saturday? |\n",
    "| weekday_is_sunday | Float | Was the article published on a Sunday? |\n",
    "| is_weekend | Float | Was the article published on the weekend? |\n",
    "| LDA_00 | Float | Closeness to LDA topic 0 |\n",
    "| LDA_01 | Float | Closeness to LDA topic 1 |\n",
    "| LDA_02 | Float | Closeness to LDA topic 2 |\n",
    "| LDA_03 | Float | Closeness to LDA topic 3 |\n",
    "| LDA_04 | Float | Closeness to LDA topic 4 |\n",
    "| global_subjectivity | Float | Text subjectivity |\n",
    "| global_sentiment_polarity | Float | Text sentiment polarity |\n",
    "| global_rate_positive_words | Float | Rate of positive words in the content |\n",
    "| global_rate_negative_words | Float | Rate of negative words in the content |\n",
    "| rate_positive_words | Float | Rate of positive words among non-neutral tokens |\n",
    "| rate_negative_words | Float | Rate of negative words among non-neutral tokens |\n",
    "| avg_positive_polarity | Float | Avg polarity of positive words |\n",
    "| min_positive_polarity | Float | Min polarity of positive words |\n",
    "| max_positive_polarity | Float | Max polarity of positive words |\n",
    "| avg_negative_polarity | Float | Avg polarity of positive words |\n",
    "| min_negative_polarity | Float | Min polarity of positive words |\n",
    "| max_negative_polarity | Float | Max polarity of positive words |\n",
    "| title_subjectivity | Float | Title subjectivity |\n",
    "| title_sentiment_polarity | Float | Title polarity |\n",
    "| abs_title_subjectivity | Float | Absolute subjectivity level |\n",
    "| abs_title_sentiment_polarity | Float | Absolute polarity level |\n",
    "| Number of shares (target) | Integer | Number of Article Shares (tweets, shares, etc...)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "Phillip\n",
    "* Calculating Acc, Rec, F-measure, Negative Predictive Value, Specificity etc... from confusion matrix\n",
    "\n",
    "Gino - Saturday Afternoon\n",
    "\n",
    "Final attributes from dimension reduction:\n",
    "['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']\n",
    "\n",
    "* 2 Tasks: Popularity (classification) and Share # (Regression)\n",
    "* Regression - Linear, Lasso, Ridge\n",
    "* Classification - Logistic Regression, K nearest Neighbors, Random Forest\n",
    "\n",
    "\n",
    "* Accuracy\n",
    "Accuracy : the proportion of the total number of predictions that were correct.\n",
    "(a + d) / (a + b + c + d)\n",
    "\n",
    "* Precision\n",
    "Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "a / (a + b)\n",
    "Negative Predictive Value : the proportion of negative cases that were correctly identified.\n",
    "d / (c + d)\n",
    "\n",
    "\n",
    "\n",
    "* Recall\n",
    "Recall : the proportion of actual positive cases which are correctly identified.\n",
    "a / (a + c)\n",
    "\n",
    "Recall could be our primary go to metric as it evaluates the context of identifying online articles which fall between High to Medium popularity or Medium to Low popularity(ex. articles that are on the border line between popularity classes)\n",
    "\n",
    "* F-Measure\n",
    "the proportion of actual negative cases which are correctly identified.\n",
    "d / (b + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import metrics to collect metrics for each modelS\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "#Separating data sets for each task\n",
    "\n",
    "#task 1 Popularity Classification\n",
    "#df_task1 = df_imputed[['popularity','n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']].copy()\n",
    "df_task1=df_imputed\n",
    "\n",
    "#task 2 isWeekend Classification\n",
    "df_task2=df_imputed\n",
    "df_task2 = df_task2.drop(['monday', 'tuesday','wednesday','thursday','friday','saturday','sunday'], axis=1)\n",
    " \n",
    "\n",
    "#Task 1\n",
    "#Create list to store datapoints for accuracy, precision, recall, F-measure from each of the model\n",
    "accuracy_task1 = []\n",
    "precision_task1 = []\n",
    "recall_task1 = []\n",
    "fmeasure_task1 = []\n",
    "\n",
    "#Task 2\n",
    "#Create list to store datapoints for accuracy, precision, recall, F-measure from each of the model\n",
    "accuracy_task2 = []\n",
    "precision_task2 = []\n",
    "recall_task2 = []\n",
    "fmeasure_task2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2\n",
    "Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "Gino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our approach we will be using Stratified 10-Fold Cross Validation for our analysis. \n",
    "This approach will provide our analysis with the high degree of precision, it is most appropriate for us to use in our analysis due to the characteristic of our data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "Gino\n",
    "\n",
    "* For Regression make sure popularity field is excluded and for classification make sure Share # is excluded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Popularity (classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stratified 10 folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_task1_temp=df_task1\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'popularity' in df_task1_temp:\n",
    "    y_1 = df_task1_temp['popularity'].values # get the labels we want\n",
    "    del df_task1_temp['popularity'] # get rid of the class label\n",
    "    X_1 = df_task1_temp.values # use everything else to predict!\n",
    "    \n",
    "yhat_1 = np.zeros(y.shape)\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "                         \n",
    "print(cv_object)\n",
    "cv_object.get_n_splits(X_1,y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f195cdfe8c0e4152902fac2de219b252"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "#penalty is set to default, which is 12.\n",
    "def lr_explor(cost):\n",
    "    lr_task1 = LogisticRegression(C=cost, class_weight=None)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_1,y_1):\n",
    "        lr_task1.fit(X_1[train],y_1[train])\n",
    "        yhat_1[test] = lr_task1.predict(X_1[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y_1, yhat_1)\n",
    "    recall = mt.recall_score(y_1, yhat_1)\n",
    "    precision = mt.precision_score(y_1, yhat_1)\n",
    "    f = mt.f1_score(y_1, yhat_1)\n",
    "\n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task1.append(['Logistic Regression',acc])\n",
    "recall_task1.append(['Logistic Regression',recall])\n",
    "precision_task1.append(['Logistic Regression',precision])\n",
    "fmeasure_task1.append(['Logistic Regression',f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-465-3c2fd6153a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mbest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# As a team we setup KNN Classifier iterator to to determine the accurate number of nearest neighbours\n",
    "# the highest iterations we are planning was 30, to get the best accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "counter = 1;\n",
    "best_accuracy= 0.0;\n",
    "kVal = 1;\n",
    "while counter <= 30:\n",
    "    clf = KNeighborsClassifier(n_neighbors=counter)\n",
    "    clf.fit(X_1[train],y_1[train])\n",
    "    acc = clf.score(X_1[test],y_1[test]);\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc;\n",
    "        kVal = counter;\n",
    "    counter += 1;\n",
    "neighbors=kVal\n",
    "print(\"Best Accuracy returned by the classifier is: {0:.4f}%\".format(best_accuracy*100),\"with k value of:\",kVal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual trainning and testing of the model begins\n",
    "print(\"The best k value:\", neighbors)\n",
    "knn_task1 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X_1,y_1):\n",
    "    knn_task1.fit(X_1[train],y_1[train])\n",
    "    yhat_1[test] = knn_task1.predict(X_1[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y_1, yhat_1)\n",
    "recall = mt.recall_score(y_1, yhat_1)\n",
    "precision = mt.precision_score(y_1, yhat_1)\n",
    "f = mt.f1_score(y_1, yhat_1)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task1.append(['KNN',acc])\n",
    "recall_task1.append(['KNNl',recall])\n",
    "precision_task1.append(['KNN',precision])\n",
    "fmeasure_task1.append(['KNN',f])\n",
    "\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def n_estimator(num):\n",
    "    # get a handle to the classifier object, which defines the type\n",
    "    rf_task1 = RandomForestClassifier(n_estimators=num, n_jobs=-1)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_1,y_1):\n",
    "        rf_task1.fit(X_1[train],y_1[train])\n",
    "        yhat_1[test] = rf_task1.predict(X_1[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y_1, yhat_1)\n",
    "    recall = mt.recall_score(y_1, yhat_1)\n",
    "    precision = mt.precision_score(y_1, yhat_1)\n",
    "    f = mt.f1_score(y_1, yhat_1)\n",
    "\n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "wd.interact(n_estimator,num=(100,150,10),__manual=True) \n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task1.append(['Random Forest',acc])\n",
    "recall_task1.append(['Random Forest',recall])\n",
    "precision_task1.append(['Random Forest',precision])\n",
    "fmeasure_task1.append(['Random Forest',f])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Weekend (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_object=None\n",
    "df_task2_temp=df_task2\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'weekend' in df_task2_temp:\n",
    "    y_2 = df_task2_temp['weekend'].values # get the labels we want\n",
    "    del df_task2_temp['weekend'] # get rid of the class label\n",
    "    X_2 = df_task2_temp.values # use everything else to predict!\n",
    "    \n",
    "yhat_2 = np.zeros(y.shape)\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "                         \n",
    "print(cv_object)\n",
    "cv_object.get_n_splits(X_2,y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost_2):\n",
    "# get a handle to the classifier object, which defines the type\n",
    "#penalty is set to default, which is 12.\n",
    "    lr_task2 = LogisticRegression(C=cost_2, class_weight=None)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_2,y_2):\n",
    "        lr_task2.fit(X_2[train],y_2[train])\n",
    "        yhat_2[test] = lr_task2.predict(X_2[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y_2, yhat_2)\n",
    "    recall = mt.recall_score(y_2, yhat_2)\n",
    "    precision = mt.precision_score(y_2, yhat_2)\n",
    "    f = mt.f1_score(y_2, yhat_2)\n",
    "    \n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "    \n",
    "wd.interact(lr_explor,cost_2=(0.001,5.0,0.05),__manual=True) \n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task2.append(['Logistic Regression',acc])\n",
    "recall_task2.append(['Logistic Regression',recall])\n",
    "precision_task2.append(['Logistic Regression',precision])\n",
    "fmeasure_task2.append(['Logistic Regression',f])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a team we setup KNN Classifier iterator to to determine the accurate number of nearest neighbours\n",
    "# the highest iterations we are planning was 30, to get the best accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "counter = 1;\n",
    "best_accuracy= 0.0;\n",
    "kVal = 1;\n",
    "while counter <= 30:\n",
    "    clf = KNeighborsClassifier(n_neighbors=counter)\n",
    "    clf.fit(X_2[train],y_2[train])\n",
    "    acc = clf.score(X_2[test],y_2[test]);\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc;\n",
    "        kVal = counter;\n",
    "    counter += 1;\n",
    "neighbors=kVal\n",
    "print(\"Best Accuracy returned by the classifier is: {0:.4f}%\".format(best_accuracy*100),\"with k value of:\",kVal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual trainning and testing of the model begins\n",
    "print(\"The best k value:\", neighbors)\n",
    "knn_task2 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X_2,y_2):\n",
    "    knn_task2.fit(X_2[train],y_2[train])\n",
    "    yhat_2[test] = knn_task2.predict(X_2[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y_2, yhat_2)\n",
    "recall = mt.recall_score(y_2, yhat_2)\n",
    "precision = mt.precision_score(y_2, yhat_2)\n",
    "f = mt.f1_score(y_2, yhat_2)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task2.append(['KNN',acc])\n",
    "recall_task2.append(['KNN',recall])\n",
    "precision_task2.append(['KNN',precision])\n",
    "fmeasure_task2.append(['KNN',f])\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def n_estimator(num):\n",
    "    # get a handle to the classifier object, which defines the type\n",
    "    rf_task2 = RandomForestClassifier(n_estimators=num, n_jobs=-1)\n",
    "\n",
    "    # iterate through and get predictions for each row in yhat\n",
    "    for train, test in cv_object.split(X_2,y_2):\n",
    "        rf_task2.fit(X_2[train],y_2[train])\n",
    "        yhat_2[test] = rf_task2.predict(X_2[test])\n",
    "\n",
    "    #evaluation metrics   \n",
    "    acc = mt.accuracy_score(y, yhat)\n",
    "    recall = mt.recall_score(y, yhat)\n",
    "    precision = mt.precision_score(y, yhat)\n",
    "    f = mt.f1_score(y, yhat)\n",
    "\n",
    "    #results in percentage\n",
    "    print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "    print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "    print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "    print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n",
    "wd.interact(n_estimator,num=(100,150,10),__manual=True) \n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_task2.append(['Random Forest',acc])\n",
    "recall_task2.append(['Random Forest',recall])\n",
    "precision_task2.append(['Random Forest',precision])\n",
    "fmeasure_task2.append(['Random Forest',f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "In this section the results of the fitted models for each task will be examined.  The resultant accuracy, recall, precision and f-measure metrics for classification task one (popularity) can be seen in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Task 1 (Popularity) Metrics\")\n",
    "print(\"\\nAccuracy\")\n",
    "print(tabulate(accuracy_task1))\n",
    "print(\"\\nRecall\")\n",
    "print(tabulate(recall_task1))              \n",
    "print(\"\\nPrecision\")\n",
    "print(tabulate(precision_task1))              \n",
    "print(\"\\nF-Measure\")\n",
    "print(tabulate(fmeasure_task1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar plot below shows these metrics represented graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = 3\n",
    "\n",
    "accuracy_task1_plt = (accuracy_task1[0][1], accuracy_task1[1][1], accuracy_task1[2][1])\n",
    "recall_task1_plt = (recall_task1[0][1], recall_task1[1][1], recall_task1[2][1])\n",
    "precision_task1_plt = (precision_task1[0][1], precision_task1[1][1], precision_task1[2][1])\n",
    "fmeasure_task1_plt = (fmeasure_task1[0][1], fmeasure_task1[1][1], fmeasure_task1[2][1])\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.4\n",
    "rects1 = plt.bar(index, accuracy_task1_plt, bar_width, alpha=opacity, color='b', label='Accuracy')\n",
    "rects2 = plt.bar(index + bar_width, recall_task1_plt, bar_width, alpha=opacity, color='r', label='Recall')\n",
    "rects3 = plt.bar(index + bar_width + bar_width, precision_task1_plt, bar_width, alpha=opacity, color='y', label='Precision')\n",
    "rects4 = plt.bar(index + bar_width + bar_width + bar_width, fmeasure_task1_plt, bar_width, alpha=opacity, color='g', label='F-Measure')\n",
    "\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Classification Task 1 (Popularity)')\n",
    "plt.xticks(index + bar_width / 2, ('Logistic Regression', 'KNN', 'Random Forest'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "The resultant accuracy, recall, precision and f-measure metrics for classification task two (weekend) can be seen in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Task 2 (Weekend) Metrics\")\n",
    "print(\"\\nAccuracy\")\n",
    "print(tabulate(accuracy_task2))\n",
    "print(\"\\nRecall\")\n",
    "print(tabulate(recall_task2))              \n",
    "print(\"\\nPrecision\")\n",
    "print(tabulate(precision_task2))              \n",
    "print(\"\\nF-Measure\")\n",
    "print(tabulate(fmeasure_task2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar plot below shows these metrics represented graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = 3\n",
    "accuracy_task2_plt = (accuracy_task2[0][1], accuracy_task2[1][1], accuracy_task2[2][1])\n",
    "recall_task2_plt = (recall_task2[0][1], recall_task2[1][1], recall_task2[2][1])\n",
    "precision_task2_plt = (precision_task2[0][1], precision_task2[1][1], precision_task2[2][1])\n",
    "fmeasure_task2_plt = (fmeasure_task2[0][1], fmeasure_task2[1][1], fmeasure_task2[2][1])\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "rects1 = plt.bar(index, accuracy_task2_plt, bar_width, alpha=opacity, color='b', label='Accuracy')\n",
    "rects2 = plt.bar(index + bar_width, recall_task2_plt, bar_width, alpha=opacity, color='r', label='Recall')\n",
    "rects3 = plt.bar(index + bar_width + bar_width, precision_task2_plt, bar_width, alpha=opacity, color='y', label='Precision')\n",
    "rects4 = plt.bar(index + bar_width + bar_width + bar_width, fmeasure_task2_plt, bar_width, alpha=opacity, color='g', label='F-Measure')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Classification Task 2 (Weekend)')\n",
    "plt.xticks(index + bar_width / 2, ('Logistic', 'KNN', 'Random Forest'))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "John\n",
    "Notebook - Grand Puba Notebook\n",
    "Rsquared\n",
    "\n",
    "In this section the models used in the analysis of tasks one and two will be compared. While accuracy will be one of the primary focuses, metrics related to the cost of estimating incorectly will also be considered.  These include recall, precision and F-Measure.  If these metrics are ignored there is an increased risk of failing to detect alternate class examples which may cause any perceived accuracy to be misleading.  Higher prceision ensures lower flase positives are detected and higher recall ensure lower false negatives are detected.  The two can be combined into the effective F-measure score where higher scores have lower false negatives and false positives.  Sample size can also have an effect on the suitability of the model.  Smaller sample sizes can easily result in a model with high bias with low variance due to the lack of equally distributed and representative samples.  Large sample sizes provide a better scenario to ensure bias is minimized.  The dataset in this analysis with over 30000 records is sufficient.  The models were fits taking into account the high variance present in order to not overfit the model.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "\n",
    "Task 1\n",
    "Log Regression\n",
    "\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "\n",
    "\n",
    "John\n",
    "* Discuss weights in more detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_task1.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_task1, X_1, y=y_1, cv=cv_object) # this also can help with parallelism\n",
    "weights = lr_task1.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "weights = pd.Series(lr_task1.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_task2, X_1, y=y_1, cv=cv_object) # this also can help with parallelism\n",
    "weights = lr_task2.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "weights = pd.Series(lr_task1.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance with Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#accuracies = cross_val_score(rf_task1, X_1, y=y_1, cv=cv_object) # this also can help with parallelism\n",
    "#weights = rf_task1.feature_importances_ # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "weights = pd.Series(rf_task1.feature_importances_,index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#accuracies = cross_val_score(rf_task1, X_1, y=y_1, cv=cv_object) # this also can help with parallelism\n",
    "#weights = rf_task2.feature_importances_ # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "weights = pd.Series(rf_task2.feature_importances_,index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#accuracies = cross_val_score(knn_task1, X_1, y=y_1, cv=cv_object) # this also can help with parallelism\n",
    "test = [[np.random.uniform(-1, 1) for _ in range(len(X_1[1]))]]\n",
    "\n",
    "neighbors, distances = knn_task1.kneighbors(test)\n",
    "print(neighbors)\n",
    "for d in distances:\n",
    "    weight = 1.0/d\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_task1.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? \n",
    "\n",
    "Scott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "\n",
    "Scott\n",
    "\n",
    "Possible Analysis:\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "Neaural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "0687b94527184dfb89c70cf1ff999470": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "81fda56fe4074db797d88e7d9d317fd4": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "c95a2cf90dad42539cf92903050455f1": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "f2833ada527b49768c0cced447e51548": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
