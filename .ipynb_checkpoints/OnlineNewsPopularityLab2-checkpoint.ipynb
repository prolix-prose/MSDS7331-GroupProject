{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Online News Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Created by Phillip Efthimion, Scott Payne, Gino Varghese and John Blevins**\n",
    "\n",
    "*MSDS 7331 Data Mining - Section 403 - Lab 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1\t\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "Phillip\n",
    "* Copy data preparation steps\n",
    "* Copy Dimension Reduction and Scaling from Minilab (PCA)- this pickup up in Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7346 entries, 4 to 39639\n",
      "Data columns (total 54 columns):\n",
      "n_tokens_title                  7346 non-null float64\n",
      "n_tokens_content                7346 non-null float64\n",
      "n_unique_tokens                 7346 non-null float64\n",
      "n_non_stop_words                7346 non-null float64\n",
      "n_non_stop_unique_tokens        7346 non-null float64\n",
      "num_hrefs                       7346 non-null float64\n",
      "num_self_hrefs                  7346 non-null float64\n",
      "num_imgs                        7346 non-null float64\n",
      "num_videos                      7346 non-null float64\n",
      "average_token_length            7346 non-null float64\n",
      "num_keywords                    7346 non-null float64\n",
      "kw_min_min                      7346 non-null float64\n",
      "kw_max_min                      7346 non-null float64\n",
      "kw_avg_min                      7346 non-null float64\n",
      "kw_min_max                      7346 non-null float64\n",
      "kw_max_max                      7346 non-null float64\n",
      "kw_avg_max                      7346 non-null float64\n",
      "kw_min_avg                      7346 non-null float64\n",
      "kw_max_avg                      7346 non-null float64\n",
      "kw_avg_avg                      7346 non-null float64\n",
      "self_reference_min_shares       7346 non-null float64\n",
      "self_reference_max_shares       7346 non-null float64\n",
      "self_reference_avg_sharess      7346 non-null float64\n",
      "monday                          7346 non-null float64\n",
      "tuesday                         7346 non-null float64\n",
      "wednesday                       7346 non-null float64\n",
      "thursday                        7346 non-null float64\n",
      "friday                          7346 non-null float64\n",
      "saturday                        7346 non-null float64\n",
      "sunday                          7346 non-null float64\n",
      "weekend                         7346 non-null float64\n",
      "LDA_00                          7346 non-null float64\n",
      "LDA_01                          7346 non-null float64\n",
      "LDA_02                          7346 non-null float64\n",
      "LDA_03                          7346 non-null float64\n",
      "LDA_04                          7346 non-null float64\n",
      "global_subjectivity             7346 non-null float64\n",
      "global_sentiment_polarity       7346 non-null float64\n",
      "global_rate_positive_words      7346 non-null float64\n",
      "global_rate_negative_words      7346 non-null float64\n",
      "rate_positive_words             7346 non-null float64\n",
      "rate_negative_words             7346 non-null float64\n",
      "avg_positive_polarity           7346 non-null float64\n",
      "min_positive_polarity           7346 non-null float64\n",
      "max_positive_polarity           7346 non-null float64\n",
      "avg_negative_polarity           7346 non-null float64\n",
      "min_negative_polarity           7346 non-null float64\n",
      "max_negative_polarity           7346 non-null float64\n",
      "title_subjectivity              7346 non-null float64\n",
      "title_sentiment_polarity        7346 non-null float64\n",
      "abs_title_subjectivity          7346 non-null float64\n",
      "abs_title_sentiment_polarity    7346 non-null float64\n",
      "shares                          7346 non-null int64\n",
      "popularity                      7346 non-null int32\n",
      "dtypes: float64(52), int32(1), int64(1)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Import and Configure Required Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "\n",
    "# Read Online News Data\n",
    "df = pd.read_csv('data/OnlineNewsPopularity.csv')\n",
    "\n",
    "# Correct Column Names by Removing Leading Space\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "\n",
    "# Rename Columns for Ease of Display\n",
    "df = df.rename(columns={'weekday_is_monday': 'monday', 'weekday_is_tuesday': 'tuesday', 'weekday_is_wednesday': 'wednesday', 'weekday_is_thursday': 'thursday', 'weekday_is_friday': 'friday', 'weekday_is_saturday': 'saturday', 'weekday_is_sunday': 'sunday', 'is_weekend': 'weekend'})\n",
    "df = df.rename(columns={'data_channel_is_lifestyle':'lifestyle', 'data_channel_is_entertainment':'entertainment', 'data_channel_is_bus':'business', 'data_channel_is_socmed':'social_media', 'data_channel_is_tech':'technology', 'data_channel_is_world':'world'})\n",
    "\n",
    "# Encode a new \"popular\" column based on the # of shares \n",
    "# \"popular\" = 1 and \"not popular\" to 0.\n",
    "df['popularity'] = pd.qcut(df['shares'].values, 2, labels=[0,1])\n",
    "df.popularity = df.popularity.astype(np.int)\n",
    "\n",
    "# Take a subset of the data related to Technology News Articles\n",
    "dfsubset = df.loc[df['technology'] == 1]\n",
    "\n",
    "# Reassign to New Variable and remove Columns which aren't needed\n",
    "df_imputed = dfsubset\n",
    "del df_imputed['url']\n",
    "#del df_imputed['shares']\n",
    "del df_imputed['timedelta']\n",
    "del df_imputed['lifestyle']\n",
    "del df_imputed['entertainment']\n",
    "del df_imputed['business']\n",
    "del df_imputed['social_media']\n",
    "del df_imputed['technology']\n",
    "del df_imputed['world']\n",
    "\n",
    "# Display Dataframe Structure\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### including data points for evaluation section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Adjustment for Improving Accuracy\n",
    "Attributes that don't provide any explanatory value have already been removed that would have affected the fitted model.  In addition the cost can be adjusted to improve accuracy.  The cost parameter tells the model optimization how much training data you want to avoid being misclassified. The large the cost, the optimization will choose smaller-margin hyperplane, the hyperplane does a good job in getting all the points in the training data classified. Similarly a small cost value will let the optimization to look for larger-margin separating hyperplane,in this approach the hyperplane ignore some points the training data will still be linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resubmission change: 01\n",
    "ipywidget has been fixed to be interactive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-da4e413f2cc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# and here is an even shorter way of getting the accuracies for each training and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_object\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# this also can help with parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_clf' is not defined"
     ]
    }
   ],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None) # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretting weights for Logistic Regression\n",
    "\n",
    "The weights of the coefficients for logistic regressions are important for determining what attributes to include in the model. The logistic regression coefficients are used to predict the probability of an outcome, in this case, the popularity of the news article being \"popular\" or \"not popular\". A positive weight indicates that an increase in the attribute will increase the odds of the outcome being “popular”, while a negative weight indicates that an increase in the attribute will decrease the likelihood of the outcome being “popular”.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Features\n",
    "Because the attributes do not all use the same measurement scale, the magnitude of the weights does not give meaningful insight into which attributes are most important for the model. The weights need to be scaled in the same way so that the magnitudes can be compared across all the features. The standard scaler will adjust the values of each attribute to be scaled by the average and standard deviation of each feature. The linear regression model will then be fit to the scaled values and the coefficient weights will be calculated based on that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_imputed.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the regression coefficients are now scaled and the magnitudes can be compared against each other. The greatest magnitude coefficients are features that contain information about the maximum, average, and minimum number of shares for the post’s keyword (kw_max_avg, kw_min_min, kw_avg_avg). It makes sense that keywords with a higher minimum and average popularity would increase the probability of the outcome being popular. The number of links (num_hrefs) found within the article has a large positive magnitude, while the number of links to other articles on the same website(num_self_hrefs) has a large negative magnitude. It is possible that there is a collinearity problem with these features that should be explored further. The rate of positive words and rate of negative words have nearly identical magnitudes but in opposite directions, rate of positive words decreases the probability of popularity and rate of negative words increases it. The features that determine if the day is Saturday or the day is a weekend both have large positive magnitudes. Since these features are related, only including the feature indicating if the day is a weekend should not hurt the model. The last three impactful features are concerning the number of tokens, unique tokens, and nonstop unique tokens in the article. Greater number of tokens, or longer articles and nonstop unique tokens seem to increase the odds of popularity while total number of unique tokens is negative. This would indicate that longer articles with flourishes of colorful language can increase popularity while a large vocabulary in general can decrease popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph of the coefficient weights, a cut off of > 0.1 and < - 0.1 would include the highest magnitude features while removing some of the less impactful attributes. Using this threshold, the following features would be included in the model: n_tokens_content, n_unique_tokens, n_non_stop_unique_tokens, num_hrefs, num_self_hrefs, kw_min_min, kw_max_max, kw_max_avg, kw_avg_avg, weekend, global_rate_positive_words, global_rate_negative_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly import __version__\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import Bar\n",
    "# run at the start of every notebook\n",
    "\n",
    "\n",
    "init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': df_imputed.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a 0.1 threshold from the weight plot, we see that n_tokens_content, n_unique_tokens, n_non_stop_unique_tokens, num_hrefs, num_self_hrefs, kw_min_min, kw_max_max, kw_max_avg, kw_avg_avg, self_reference_avg_sharess, saturday, weekend, global_rate_positive_words, global_rate_negative_words and rate_negative_words all have the most weight in predicting popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df_imputed[['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_max', 'kw_max_avg', 'kw_avg_avg', 'self_reference_avg_sharess', 'saturday', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_max', 'kw_max_avg', 'kw_avg_avg', 'self_reference_avg_sharess', 'saturday', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we use a 0.1 threshold from the weight plot, we see that n_tokens_content, n_unique_tokens, n_non_stop_unique_tokens, num_hrefs, num_self_hrefs, kw_min_min, kw_max_max, kw_max_avg, kw_avg_avg,  weekend, global_rate_positive_words, global_rate_negative_words and rate_negative_words all have the most weight in predicting popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df_imputed[['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_max', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_max', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a 0.1 threshold from the weight plot, we see that n_tokens_content, n_unique_tokens, n_non_stop_unique_tokens, num_hrefs, num_self_hrefs, kw_min_min, kw_max_avg, kw_avg_avg,  weekend, global_rate_positive_words, global_rate_negative_words and rate_negative_words all have the most weight in predicting popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df_imputed[['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "Phillip\n",
    "* Describe how we created Popularity\n",
    "* Insert chart with attributes and meaning from lab 1\n",
    "The online news popularity data set utilized in this analysis is publicly accessible from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity.  The data was originally collected by Mashable based on articles published through their website between 2013 and 2015.  The data set contains 39,644 data points with 61 attributes including 58 predictive attributes, 2 non-predictive attributes and 1 target attribute.\n",
    "\n",
    "We created the ‘popularity' class variable using our data from our ‘shares’ variable. We have decided to measure how popular an article from the Mashable dataset is based on the number of shares it receives. If an article has been shared more than a requisite number of times, then it is deemed popular. We created popularity with the ‘qcut’ tool in pandas and split the ‘shares' into two categories: ‘popular’ and ‘not popular’, which are coded by 1 and 0 respectively. ‘Popularity’ is coded as a non-null integers, though most of the other variables are floats. \n",
    "\n",
    "The 2 non-predictive attributes are as follows:\n",
    "* URL of the news article\n",
    "* Number of days between article publication and dataset acquisition\n",
    "\n",
    "The explanatory attributes can be split into several different groupings as follows:\n",
    "* Word Structure and Frequency\n",
    "* Hyperlink References\n",
    "* Digital Media References (Images and Videos)\n",
    "* Channel Categorization and Keywords (content type such as lifestyle, entertainment, etc...)\n",
    "* Publication Time\n",
    "* Sentiment and Subjectivity Levels\n",
    "\n",
    "The target attribute is the number of shares a site receives which indicates the popularity of the site.  The complete list of attributes is shown in the following table:\n",
    "\n",
    "|Attribute|Data Type|Description|\n",
    "|---------|---------|-----------|\n",
    "| url | Object | URL of the article (non-predictive) |\n",
    "| timedelta | Float | Days between the article publication and the dataset acquisition (non-predictive) |\n",
    "| n_tokens_title | Float | Number of words in the title |\n",
    "| n_tokens_content | Float | Number of words in the content |\n",
    "| n_unique_tokens | Float | Rate of unique words in the content |\n",
    "| n_non_stop_words | Float | Rate of non-stop words in the content |\n",
    "| n_non_stop_unique_tokens | Float | Rate of unique non-stop words in the content |\n",
    "| num_hrefs | Float | Number of links |\n",
    "| num_self_hrefs | Float | Number of links to other articles published by Mashable |\n",
    "| num_imgs | Float | Number of images |\n",
    "| num_videos | Float | Number of videos |\n",
    "| average_token_length | Float | Average length of the words in the content |\n",
    "| num_keywords | Float | Number of keywords in the metadata |\n",
    "| data_channel_is_lifestyle | Float | Is data channel 'Lifestyle'? |\n",
    "| data_channel_is_entertainment | Float | Is data channel 'Entertainment'? |\n",
    "| data_channel_is_bus | Float | Is data channel 'Business'? |\n",
    "| data_channel_is_socmed | Float | Is data channel 'Social Media'? |\n",
    "| data_channel_is_tech | Float | Is data channel 'Tech'? |\n",
    "| data_channel_is_world | Float | Is data channel 'World'? |\n",
    "| kw_min_min | Float | Worst keyword (min) |\n",
    "| kw_max_min | Float | Worst keyword (max) |\n",
    "| kw_avg_min | Float | Worst keyword (avg) |\n",
    "| kw_min_max | Float | Best keyword (min) |\n",
    "| kw_max_max | Float | Best keyword (max) |\n",
    "| kw_avg_max | Float | Best keyword (avg) |\n",
    "| kw_min_avg | Float | Avg keyword (min) |\n",
    "| kw_max_avg | Float | Avg keyword (max) |\n",
    "| kw_avg_avg | Float | Avg keyword (avg) |\n",
    "| self_reference_min_shares | Float | Min  of referenced articles in Mashable |\n",
    "| self_reference_max_shares | Float | Max  of referenced articles in Mashable |\n",
    "| self_reference_avg_sharess | Float | Avg  of referenced articles in Mashable |\n",
    "| weekday_is_monday | Float | Was the article published on a Monday? |\n",
    "| weekday_is_tuesday | Float | Was the article published on a Tuesday? |\n",
    "| weekday_is_wednesday | Float | Was the article published on a Wednesday? |\n",
    "| weekday_is_thursday | Float | Was the article published on a Thursday? |\n",
    "| weekday_is_friday | Float | Was the article published on a Friday? |\n",
    "| weekday_is_saturday | Float | Was the article published on a Saturday? |\n",
    "| weekday_is_sunday | Float | Was the article published on a Sunday? |\n",
    "| is_weekend | Float | Was the article published on the weekend? |\n",
    "| LDA_00 | Float | Closeness to LDA topic 0 |\n",
    "| LDA_01 | Float | Closeness to LDA topic 1 |\n",
    "| LDA_02 | Float | Closeness to LDA topic 2 |\n",
    "| LDA_03 | Float | Closeness to LDA topic 3 |\n",
    "| LDA_04 | Float | Closeness to LDA topic 4 |\n",
    "| global_subjectivity | Float | Text subjectivity |\n",
    "| global_sentiment_polarity | Float | Text sentiment polarity |\n",
    "| global_rate_positive_words | Float | Rate of positive words in the content |\n",
    "| global_rate_negative_words | Float | Rate of negative words in the content |\n",
    "| rate_positive_words | Float | Rate of positive words among non-neutral tokens |\n",
    "| rate_negative_words | Float | Rate of negative words among non-neutral tokens |\n",
    "| avg_positive_polarity | Float | Avg polarity of positive words |\n",
    "| min_positive_polarity | Float | Min polarity of positive words |\n",
    "| max_positive_polarity | Float | Max polarity of positive words |\n",
    "| avg_negative_polarity | Float | Avg polarity of positive words |\n",
    "| min_negative_polarity | Float | Min polarity of positive words |\n",
    "| max_negative_polarity | Float | Max polarity of positive words |\n",
    "| title_subjectivity | Float | Title subjectivity |\n",
    "| title_sentiment_polarity | Float | Title polarity |\n",
    "| abs_title_subjectivity | Float | Absolute subjectivity level |\n",
    "| abs_title_sentiment_polarity | Float | Absolute polarity level |\n",
    "| Number of shares (target) | Integer | Number of Article Shares (tweets, shares, etc...)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "Phillip\n",
    "* Calculating Acc, Rec, F-measure, Negative Predictive Value, Specificity etc... from confusion matrix\n",
    "\n",
    "Gino - Saturday Afternoon\n",
    "\n",
    "Final attributes from dimension reduction:\n",
    "['n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']\n",
    "\n",
    "* 2 Tasks: Popularity (classification) and Share # (Regression)\n",
    "* Regression - Linear, Lasso, Ridge\n",
    "* Classification - Logistic Regression, K nearest Neighbors, Random Forest\n",
    "\n",
    "\n",
    "* Accuracy\n",
    "Accuracy : the proportion of the total number of predictions that were correct.\n",
    "(a + d) / (a + b + c + d)\n",
    "\n",
    "* Precision\n",
    "Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "a / (a + b)\n",
    "Negative Predictive Value : the proportion of negative cases that were correctly identified.\n",
    "d / (c + d)\n",
    "\n",
    "\n",
    "\n",
    "* Recall\n",
    "Recall : the proportion of actual positive cases which are correctly identified.\n",
    "a / (a + c)\n",
    "\n",
    "Recall could be our primary go to metric as it evaluates the context of identifying online articles which fall between High to Medium popularity or Medium to Low popularity(ex. articles that are on the border line between popularity classes)\n",
    "\n",
    "* F-Measure\n",
    "the proportion of actual negative cases which are correctly identified.\n",
    "d / (b + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import metrics to collect metrics for each modelS\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "#Separating data sets for each task\n",
    "\n",
    "#task 1 Popularity Classification\n",
    "df_task1 = df_imputed[['popularity','n_tokens_content', 'n_unique_tokens', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'kw_min_min', 'kw_max_avg', 'kw_avg_avg', 'weekend', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_negative_words']].copy()\n",
    "\n",
    "\n",
    "#task 2 Shares Regression\n",
    "df_task2 = df_imputed\n",
    "df_task2 = df_task2.drop(['popularity'],axis=1)\n",
    "\n",
    "\n",
    "#Create list to store datapoints for accuracy, precision, recall, F-measure from each of the model\n",
    "accuracy_dp = []\n",
    "precision_dp = []\n",
    "recall_dp = []\n",
    "fmeasure_dp = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2\n",
    "Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "Gino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our approach we will be using Stratified 10-Fold Cross Validation for our analysis. \n",
    "This approach will provide our analysis with the high degree of precision, it is most appropriate for us to use in our analysis due to the characteristic of our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified 10 folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'popularity' in df_task1:\n",
    "    y = df_task1['popularity'].values # get the labels we want\n",
    "    del df_task1['popularity'] # get rid of the class label\n",
    "    X = df_task1.values # use everything else to predict!\n",
    "    \n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "                         \n",
    "print(cv_object)\n",
    "cv_object.get_n_splits(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "Gino\n",
    "\n",
    "* For Regression make sure popularity field is excluded and for classification make sure Share # is excluded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Popularity (classification) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "#penalty is set to default, which is 12.\n",
    "lr_task1 = LogisticRegression(C=1.0, class_weight=None)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X,y):\n",
    "    lr_task1.fit(X[train],y[train])\n",
    "    yhat[test] = lr_task1.predict(X[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y, yhat)\n",
    "recall = mt.recall_score(y, yhat)\n",
    "precision = mt.precision_score(y, yhat)\n",
    "f = mt.f1_score(y, yhat)\n",
    "\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_dp.append(acc)\n",
    "recall_dp.append(recall)\n",
    "precision_dp.append(precision)\n",
    "fmeasure_dp.append(f)\n",
    "\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K nearest Neighbors, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a team we setup KNN Classifier iterator to to determine the accurate number of nearest neighbours\n",
    "# the highest iterations we are planning was 30, to get the best accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "counter = 1;\n",
    "best_accuracy= 0.0;\n",
    "kVal = 1;\n",
    "while counter <= 30:\n",
    "    clf = KNeighborsClassifier(n_neighbors=counter)\n",
    "    clf.fit(X[train],y[train])\n",
    "    acc = clf.score(X[test],y[test]);\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc;\n",
    "        kVal = counter;\n",
    "    counter += 1;\n",
    "neighbors=kVal\n",
    "print(\"Best Accuracy returned by the classifier is: {0:.4f}%\".format(best_accuracy*100),\"with k value of:\",kVal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual trainning and testing of the model begins\n",
    "print(\"The best k value:\", neighbors)\n",
    "knn_task1 = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X,y):\n",
    "    knn_task1.fit(X[train],y[train])\n",
    "    yhat[test] = knn_task1.predict(X[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y, yhat)\n",
    "recall = mt.recall_score(y, yhat)\n",
    "precision = mt.precision_score(y, yhat)\n",
    "f = mt.f1_score(y, yhat)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_dp.append(acc)\n",
    "recall_dp.append(recall)\n",
    "precision_dp.append(precision)\n",
    "fmeasure_dp.append(f)\n",
    "\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "rf_task1 = RandomForestClassifier(n_estimators=150, n_jobs=-1)\n",
    "\n",
    "# iterate through and get predictions for each row in yhat\n",
    "for train, test in cv_object.split(X,y):\n",
    "    rf_task1.fit(X[train],y[train])\n",
    "    yhat[test] = rf_task1.predict(X[test])\n",
    "\n",
    "#evaluation metrics   \n",
    "acc = mt.accuracy_score(y, yhat)\n",
    "recall = mt.recall_score(y, yhat)\n",
    "precision = mt.precision_score(y, yhat)\n",
    "f = mt.f1_score(y, yhat)\n",
    "\n",
    "#adding evaluation metrics to list for further analysis between models\n",
    "accuracy_dp.append(acc)\n",
    "recall_dp.append(recall)\n",
    "precision_dp.append(precision)\n",
    "fmeasure_dp.append(f)\n",
    "\n",
    "\n",
    "#results in percentage\n",
    "print(\"Accuracy of the model: {0:.4f}%\".format(acc*100))\n",
    "print(\"Recall of the model: {0:.4f}%\".format(recall*100))\n",
    "print(\"Precision of the model: {0:.4f}%\".format(precision*100))\n",
    "print(\"F-measure of the model: {0:.4f}%\".format(f*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Number of Shares (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'shares' in df_task2:\n",
    "    y = df_task2['shares'].values # get the labels we want\n",
    "    del df_task2['shares'] # get rid of the class label\n",
    "    X = df_task2.values # use everything else to predict!\n",
    "\n",
    "#X =  np.array([np.concatenate((v,[1])) for v in df_task2.values])\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "#cv_object = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "cv_object = KFold(len(X),n_folds=10)\n",
    "\n",
    "print(cv_object)\n",
    "#cv_object.get_n_splits(X,y)\n",
    "\n",
    "lr_task2 = LinearRegression()\n",
    "\n",
    "xval_error = 0\n",
    "for train, test in cv_object:\n",
    "    lr_task2.fit(X[train], y[train])\n",
    "    p = lr_task2.predict(X[test])\n",
    "    e = p-y[test]\n",
    "    xval_error += np.dot(e,e)\n",
    "\n",
    "rmse_10cv = np.sqrt(xval_error/len(X))\n",
    "\n",
    "\n",
    "\n",
    "method_name = 'Simple Linear Regression'\n",
    "print('Method: %s' %method_name)\n",
    "print('Root Mean Square Error on training: %.4f' %rmse_train)\n",
    "print('Root Mean Square Error on 10-fold CV: %.4f' %rmse_10cv)\n",
    "print(mt.r2_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "lr_task2 = Lasso(fit_intercept=True,alpha=0.5)\n",
    "\n",
    "xval_error = 0\n",
    "for train, test in cv_object:\n",
    "    lr_task2.fit(X[train], y[train])\n",
    "    p = lr_task2.predict(X[test])\n",
    "    e = p-y[test]\n",
    "    xval_error += np.dot(e,e)\n",
    "\n",
    "rmse_10cv = np.sqrt(xval_error/len(X))\n",
    "\n",
    "\n",
    "\n",
    "method_name = 'Simple Linear Regression'\n",
    "print('Method: %s' %method_name)\n",
    "print('Root Mean Square Error on training: %.4f' %rmse_train)\n",
    "print('Root Mean Square Error on 10-fold CV: %.4f' %rmse_10cv)\n",
    "print(mt.r2_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "lr_task2 = Ridge(fit_intercept=True,alpha=0.5)\n",
    "\n",
    "xval_error = 0\n",
    "for train, test in cv_object:\n",
    "    lr_task2.fit(X[train], y[train])\n",
    "    p = lr_task2.predict(X[test])\n",
    "    e = p-y[test]\n",
    "    xval_error += np.dot(e,e)\n",
    "\n",
    "rmse_10cv = np.sqrt(xval_error/len(X))\n",
    "\n",
    "\n",
    "\n",
    "method_name = 'Simple Linear Regression'\n",
    "print('Method: %s' %method_name)\n",
    "print('Root Mean Square Error on training: %.4f' %rmse_train)\n",
    "print('Root Mean Square Error on 10-fold CV: %.4f' %rmse_10cv)\n",
    "print(mt.r2_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "John\n",
    "* For Regression just compare attributes selected which are significantly\n",
    "* For Classification determine accuracy, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "John\n",
    "Notebook - Grand Puba Notebook\n",
    "Rsquared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "John\n",
    "* Discuss weights in more detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? \n",
    "\n",
    "Scott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "\n",
    "Scott\n",
    "\n",
    "Possible Analysis:\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "Neaural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
